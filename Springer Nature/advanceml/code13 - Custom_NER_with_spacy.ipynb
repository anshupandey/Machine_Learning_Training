{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom NER with spacy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rbIqf6AhfKG"
      },
      "source": [
        "#load the pre-existing spacy model you want to use and get the ner pipeline throughget_pipe() method.\n",
        "# Import and load the spacy model\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\") \n",
        "\n",
        "# Getting the ner component\n",
        "ner=nlp.get_pipe('ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKuGM3RdhrTX"
      },
      "source": [
        "The format of the training data is a list of tuples. Each tuple contains the example text and a dictionary. The dictionary will have the key entities , that stores the start and end indices along with the label of the entitties present in the text.\n",
        "\n",
        "\n",
        "For example , To pass “Pizza is a common fast food” as example the format will be : (\"Pizza is a common fast food\",{\"entities\" : [(0, 5, \"FOOD\")]})"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GONop8gEhi6z"
      },
      "source": [
        "# New label to add\n",
        "LABEL = \"FOOD\"\n",
        "\n",
        "# Training examples in the required format\n",
        "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
        "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
        "              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\n",
        "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
        "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
        "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
        "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
        "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
        "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
        "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
        "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
        "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
        "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
        "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
        "           ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs45kJPZhwAn"
      },
      "source": [
        "# Add the new label to ner\n",
        "ner.add_label(LABEL)\n",
        "\n",
        "# Resume training\n",
        "optimizer = nlp.resume_training()\n",
        "move_names = list(ner.move_names)\n",
        "\n",
        "# List of pipes you want to train\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "\n",
        "# List of pipes which should remain unaffected in training\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y10KaGiRiDbS"
      },
      "source": [
        "For each iteration , the model or ner is update through the nlp.update() command. Parameters of nlp.update() are :\n",
        "\n",
        "- docs : This expects a batch of texts as input. You can pass each batch to the zip method , which will return you batches of text and annotations.\n",
        "`\n",
        "- sgd : You have to pass the optimizer that was returned by resume_training() here.\n",
        "\n",
        "- golds : You can pass the annotations we got through zip method here\n",
        "\n",
        "- drop : This represents the dropout rate.\n",
        "\n",
        "- losses: A dictionary to hold the losses against each pipeline component. Create an empty dictionary and pass it here.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfc-jeNxh2Yl",
        "outputId": "48242926-dc50-4444-d8c4-361507915613"
      },
      "source": [
        "# Importing requirements\n",
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "\n",
        "# Begin training by disabling other pipeline components\n",
        "with nlp.disable_pipes(*other_pipes) :\n",
        "\n",
        "  sizes = compounding(1.0, 4.0, 1.001)\n",
        "  # Training for 30 iterations     \n",
        "  for itn in range(30):\n",
        "    # shuffle examples before training\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    # batch up the examples using spaCy's minibatch\n",
        "    batches = minibatch(TRAIN_DATA, size=sizes)\n",
        "    # ictionary to store losses\n",
        "    losses = {}\n",
        "    for batch in batches:\n",
        "      texts, annotations = zip(*batch)\n",
        "      # Calling update() over the iteration\n",
        "      nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
        "      print(\"Losses\", losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses {'ner': 10.305896391102578}\n",
            "Losses {'ner': 15.879611501062755}\n",
            "Losses {'ner': 22.412719465258622}\n",
            "Losses {'ner': 26.601724364264417}\n",
            "Losses {'ner': 36.18511471099268}\n",
            "Losses {'ner': 40.138153814711444}\n",
            "Losses {'ner': 43.623866257155825}\n",
            "Losses {'ner': 52.20846163936942}\n",
            "Losses {'ner': 56.53697416909692}\n",
            "Losses {'ner': 60.478709640858355}\n",
            "Losses {'ner': 65.12382746411241}\n",
            "Losses {'ner': 73.78020515019684}\n",
            "Losses {'ner': 76.73062296290789}\n",
            "Losses {'ner': 79.79987325598393}\n",
            "Losses {'ner': 6.229953083391138}\n",
            "Losses {'ner': 12.403696040804334}\n",
            "Losses {'ner': 18.41340605467076}\n",
            "Losses {'ner': 21.374699432879567}\n",
            "Losses {'ner': 26.974140422509805}\n",
            "Losses {'ner': 32.38149624420126}\n",
            "Losses {'ner': 35.26578252364636}\n",
            "Losses {'ner': 39.089769519254624}\n",
            "Losses {'ner': 43.62643986227369}\n",
            "Losses {'ner': 51.38964396514377}\n",
            "Losses {'ner': 55.76489518777846}\n",
            "Losses {'ner': 60.985744701786025}\n",
            "Losses {'ner': 69.13893448011396}\n",
            "Losses {'ner': 71.58105947761243}\n",
            "Losses {'ner': 3.2458255933597684}\n",
            "Losses {'ner': 6.7809919491410255}\n",
            "Losses {'ner': 12.571740610255802}\n",
            "Losses {'ner': 22.017320258273685}\n",
            "Losses {'ner': 27.698782548039162}\n",
            "Losses {'ner': 31.97773704255087}\n",
            "Losses {'ner': 35.53834629427729}\n",
            "Losses {'ner': 41.99205494016496}\n",
            "Losses {'ner': 47.884613756985345}\n",
            "Losses {'ner': 52.88375890111638}\n",
            "Losses {'ner': 56.56944344049407}\n",
            "Losses {'ner': 60.756519010254124}\n",
            "Losses {'ner': 64.18090656838467}\n",
            "Losses {'ner': 67.76246984555473}\n",
            "Losses {'ner': 2.699409655528143}\n",
            "Losses {'ner': 5.285480437101796}\n",
            "Losses {'ner': 12.384445366682485}\n",
            "Losses {'ner': 17.77884568576701}\n",
            "Losses {'ner': 23.98402692680247}\n",
            "Losses {'ner': 31.93328508012928}\n",
            "Losses {'ner': 33.98886690667132}\n",
            "Losses {'ner': 41.12797690561274}\n",
            "Losses {'ner': 47.689296938537154}\n",
            "Losses {'ner': 50.40215755143436}\n",
            "Losses {'ner': 51.58514186722459}\n",
            "Losses {'ner': 52.87966781169234}\n",
            "Losses {'ner': 55.01669721923827}\n",
            "Losses {'ner': 60.11437991693674}\n",
            "Losses {'ner': 4.717488158494234}\n",
            "Losses {'ner': 8.510663151741028}\n",
            "Losses {'ner': 12.267164553515613}\n",
            "Losses {'ner': 13.640320144033467}\n",
            "Losses {'ner': 13.757348343868216}\n",
            "Losses {'ner': 18.109204964850505}\n",
            "Losses {'ner': 20.524609638749098}\n",
            "Losses {'ner': 23.71212037876103}\n",
            "Losses {'ner': 29.230940048721095}\n",
            "Losses {'ner': 33.425403633991664}\n",
            "Losses {'ner': 37.03908999813575}\n",
            "Losses {'ner': 39.68123557502258}\n",
            "Losses {'ner': 45.30220579625893}\n",
            "Losses {'ner': 47.08247868986655}\n",
            "Losses {'ner': 1.0410435096018773}\n",
            "Losses {'ner': 4.613021765417216}\n",
            "Losses {'ner': 6.394154858888214}\n",
            "Losses {'ner': 10.408575546007341}\n",
            "Losses {'ner': 15.4176760496739}\n",
            "Losses {'ner': 17.73096809898925}\n",
            "Losses {'ner': 21.056569869531813}\n",
            "Losses {'ner': 24.456829992956045}\n",
            "Losses {'ner': 26.531351623900264}\n",
            "Losses {'ner': 33.108532031230425}\n",
            "Losses {'ner': 35.49214115474388}\n",
            "Losses {'ner': 36.74276293215007}\n",
            "Losses {'ner': 41.28842870967128}\n",
            "Losses {'ner': 44.34605793454466}\n",
            "Losses {'ner': 0.17514715145807713}\n",
            "Losses {'ner': 5.0931708395219175}\n",
            "Losses {'ner': 10.328946887442726}\n",
            "Losses {'ner': 14.503108314165729}\n",
            "Losses {'ner': 18.257820370068657}\n",
            "Losses {'ner': 28.231373676491785}\n",
            "Losses {'ner': 31.50672779903516}\n",
            "Losses {'ner': 32.706321888243565}\n",
            "Losses {'ner': 39.41441740897835}\n",
            "Losses {'ner': 41.92449050428422}\n",
            "Losses {'ner': 44.02179284741169}\n",
            "Losses {'ner': 46.24450347407719}\n",
            "Losses {'ner': 50.10066891576162}\n",
            "Losses {'ner': 52.568561529489216}\n",
            "Losses {'ner': 3.310638072231086}\n",
            "Losses {'ner': 8.84305791585939}\n",
            "Losses {'ner': 10.98797576193465}\n",
            "Losses {'ner': 13.963644869356358}\n",
            "Losses {'ner': 19.628636894478404}\n",
            "Losses {'ner': 23.281484415427258}\n",
            "Losses {'ner': 25.37716762592754}\n",
            "Losses {'ner': 25.434588714961137}\n",
            "Losses {'ner': 30.905733664862055}\n",
            "Losses {'ner': 34.04218802169635}\n",
            "Losses {'ner': 36.75129475859285}\n",
            "Losses {'ner': 40.69912303653837}\n",
            "Losses {'ner': 46.79011917230673}\n",
            "Losses {'ner': 51.31921795057133}\n",
            "Losses {'ner': 3.011006352317054}\n",
            "Losses {'ner': 7.939336237323005}\n",
            "Losses {'ner': 11.546943815716077}\n",
            "Losses {'ner': 15.149871947534848}\n",
            "Losses {'ner': 23.65491671889322}\n",
            "Losses {'ner': 25.659133355031372}\n",
            "Losses {'ner': 32.56263379276788}\n",
            "Losses {'ner': 40.87759951771295}\n",
            "Losses {'ner': 46.256696608281345}\n",
            "Losses {'ner': 54.56404222211859}\n",
            "Losses {'ner': 54.710766964781214}\n",
            "Losses {'ner': 60.0715729083895}\n",
            "Losses {'ner': 65.38777869306796}\n",
            "Losses {'ner': 71.46096474789374}\n",
            "Losses {'ner': 4.0185187458992}\n",
            "Losses {'ner': 8.627609370276332}\n",
            "Losses {'ner': 14.36397003941238}\n",
            "Losses {'ner': 17.545370975509286}\n",
            "Losses {'ner': 23.485365549102426}\n",
            "Losses {'ner': 27.12630263157189}\n",
            "Losses {'ner': 31.485694797709584}\n",
            "Losses {'ner': 34.77091980166733}\n",
            "Losses {'ner': 42.256096861790866}\n",
            "Losses {'ner': 46.07150499103591}\n",
            "Losses {'ner': 49.5194616089575}\n",
            "Losses {'ner': 49.92357834702125}\n",
            "Losses {'ner': 51.832007589691784}\n",
            "Losses {'ner': 54.122746267424795}\n",
            "Losses {'ner': 3.698506658169208}\n",
            "Losses {'ner': 6.473832681047497}\n",
            "Losses {'ner': 9.394953085313318}\n",
            "Losses {'ner': 15.79256196154165}\n",
            "Losses {'ner': 17.990343827957986}\n",
            "Losses {'ner': 23.660671299527166}\n",
            "Losses {'ner': 25.708085772319464}\n",
            "Losses {'ner': 29.966510339610977}\n",
            "Losses {'ner': 34.778370549873216}\n",
            "Losses {'ner': 36.959325791031006}\n",
            "Losses {'ner': 37.04815912088088}\n",
            "Losses {'ner': 45.0271058066719}\n",
            "Losses {'ner': 47.499820312761585}\n",
            "Losses {'ner': 49.35649101177114}\n",
            "Losses {'ner': 2.329436473781243}\n",
            "Losses {'ner': 9.24009973811917}\n",
            "Losses {'ner': 12.228777676064055}\n",
            "Losses {'ner': 18.22641150743584}\n",
            "Losses {'ner': 24.032614835508866}\n",
            "Losses {'ner': 29.349185296305222}\n",
            "Losses {'ner': 35.813458420321695}\n",
            "Losses {'ner': 40.166907196704415}\n",
            "Losses {'ner': 43.35519833334547}\n",
            "Losses {'ner': 48.02253021609795}\n",
            "Losses {'ner': 52.99723031817484}\n",
            "Losses {'ner': 55.02423901742077}\n",
            "Losses {'ner': 58.7976067286545}\n",
            "Losses {'ner': 65.5227496736079}\n",
            "Losses {'ner': 7.512139150872827}\n",
            "Losses {'ner': 13.281240007803717}\n",
            "Losses {'ner': 19.746322909837545}\n",
            "Losses {'ner': 22.535280648293337}\n",
            "Losses {'ner': 27.44730741125568}\n",
            "Losses {'ner': 31.3084616667129}\n",
            "Losses {'ner': 36.10507726450487}\n",
            "Losses {'ner': 38.27016201769038}\n",
            "Losses {'ner': 43.867686561914525}\n",
            "Losses {'ner': 48.52241399912782}\n",
            "Losses {'ner': 52.92580443276711}\n",
            "Losses {'ner': 57.352299560845495}\n",
            "Losses {'ner': 59.780103208084256}\n",
            "Losses {'ner': 63.398434014578015}\n",
            "Losses {'ner': 4.5422383807599545}\n",
            "Losses {'ner': 10.302721402607858}\n",
            "Losses {'ner': 13.57768950657919}\n",
            "Losses {'ner': 16.08928424876649}\n",
            "Losses {'ner': 18.405927381129004}\n",
            "Losses {'ner': 27.077703199000098}\n",
            "Losses {'ner': 32.681018068105914}\n",
            "Losses {'ner': 38.41873983910773}\n",
            "Losses {'ner': 39.88664916460402}\n",
            "Losses {'ner': 46.16468149778666}\n",
            "Losses {'ner': 46.24403171724407}\n",
            "Losses {'ner': 54.24734243861167}\n",
            "Losses {'ner': 59.480461866769474}\n",
            "Losses {'ner': 63.43647603521822}\n",
            "Losses {'ner': 2.7348645785823464}\n",
            "Losses {'ner': 4.378148833056912}\n",
            "Losses {'ner': 7.986409925040789}\n",
            "Losses {'ner': 15.286913178977557}\n",
            "Losses {'ner': 21.34105388086755}\n",
            "Losses {'ner': 26.428634680924006}\n",
            "Losses {'ner': 28.580320924294938}\n",
            "Losses {'ner': 36.32947074040567}\n",
            "Losses {'ner': 38.92858485691977}\n",
            "Losses {'ner': 44.93508564474905}\n",
            "Losses {'ner': 52.39492674711073}\n",
            "Losses {'ner': 53.58919179280201}\n",
            "Losses {'ner': 57.73770163048903}\n",
            "Losses {'ner': 62.04710768096993}\n",
            "Losses {'ner': 3.062400974566117}\n",
            "Losses {'ner': 5.999060604954138}\n",
            "Losses {'ner': 6.057282378125819}\n",
            "Losses {'ner': 13.445267815768602}\n",
            "Losses {'ner': 15.489329536954756}\n",
            "Losses {'ner': 19.826663769359584}\n",
            "Losses {'ner': 28.415461905344273}\n",
            "Losses {'ner': 33.60055292402103}\n",
            "Losses {'ner': 37.49685345932767}\n",
            "Losses {'ner': 41.92592512041256}\n",
            "Losses {'ner': 46.19289383320529}\n",
            "Losses {'ner': 50.03375187214601}\n",
            "Losses {'ner': 53.83314946737278}\n",
            "Losses {'ner': 54.905723775980505}\n",
            "Losses {'ner': 3.3788132236513775}\n",
            "Losses {'ner': 9.140555500256596}\n",
            "Losses {'ner': 14.036406118859304}\n",
            "Losses {'ner': 17.74647696732427}\n",
            "Losses {'ner': 22.518449549068464}\n",
            "Losses {'ner': 29.15864427803899}\n",
            "Losses {'ner': 33.910701499087736}\n",
            "Losses {'ner': 36.5988931611646}\n",
            "Losses {'ner': 36.65243917424232}\n",
            "Losses {'ner': 39.58889153320342}\n",
            "Losses {'ner': 43.5189538593404}\n",
            "Losses {'ner': 46.36862391186878}\n",
            "Losses {'ner': 50.412070189078804}\n",
            "Losses {'ner': 53.76029997901628}\n",
            "Losses {'ner': 1.5596754834841704}\n",
            "Losses {'ner': 4.198654446387081}\n",
            "Losses {'ner': 11.210401580799953}\n",
            "Losses {'ner': 13.56815571928746}\n",
            "Losses {'ner': 20.99953593457758}\n",
            "Losses {'ner': 21.05935875247087}\n",
            "Losses {'ner': 22.459456855947792}\n",
            "Losses {'ner': 28.569005126650154}\n",
            "Losses {'ner': 34.4191700005249}\n",
            "Losses {'ner': 39.338272320972465}\n",
            "Losses {'ner': 39.35355631395214}\n",
            "Losses {'ner': 45.64161374410469}\n",
            "Losses {'ner': 48.22539573758695}\n",
            "Losses {'ner': 55.20714810341451}\n",
            "Losses {'ner': 4.91257444396615}\n",
            "Losses {'ner': 6.984479806385934}\n",
            "Losses {'ner': 11.415129257686203}\n",
            "Losses {'ner': 13.899821223778417}\n",
            "Losses {'ner': 18.606668682856252}\n",
            "Losses {'ner': 23.38104063094943}\n",
            "Losses {'ner': 28.10427172560594}\n",
            "Losses {'ner': 36.07141995508573}\n",
            "Losses {'ner': 39.16374490072485}\n",
            "Losses {'ner': 45.35022867133375}\n",
            "Losses {'ner': 46.4619839281404}\n",
            "Losses {'ner': 51.089740858096775}\n",
            "Losses {'ner': 58.318439350146946}\n",
            "Losses {'ner': 61.508187619005184}\n",
            "Losses {'ner': 4.828291155281477}\n",
            "Losses {'ner': 4.864137892436702}\n",
            "Losses {'ner': 5.127563159388956}\n",
            "Losses {'ner': 10.177335094136652}\n",
            "Losses {'ner': 13.378431023505982}\n",
            "Losses {'ner': 17.838521541503724}\n",
            "Losses {'ner': 24.176082958409097}\n",
            "Losses {'ner': 27.318888869776856}\n",
            "Losses {'ner': 28.822635402088054}\n",
            "Losses {'ner': 34.06809038959909}\n",
            "Losses {'ner': 39.536015311139636}\n",
            "Losses {'ner': 44.526978300438714}\n",
            "Losses {'ner': 50.41776657448463}\n",
            "Losses {'ner': 53.11988740711786}\n",
            "Losses {'ner': 7.095037542894715}\n",
            "Losses {'ner': 11.531587292042332}\n",
            "Losses {'ner': 15.222191095885591}\n",
            "Losses {'ner': 21.433496206164364}\n",
            "Losses {'ner': 24.40041681138473}\n",
            "Losses {'ner': 27.68715305441765}\n",
            "Losses {'ner': 31.251681564448518}\n",
            "Losses {'ner': 31.252022898873292}\n",
            "Losses {'ner': 38.3628158933725}\n",
            "Losses {'ner': 41.68623189056325}\n",
            "Losses {'ner': 46.66413689950514}\n",
            "Losses {'ner': 47.6374872157694}\n",
            "Losses {'ner': 49.93835712403995}\n",
            "Losses {'ner': 54.25605308816773}\n",
            "Losses {'ner': 4.292265291875083}\n",
            "Losses {'ner': 4.317447934499796}\n",
            "Losses {'ner': 11.51437309776884}\n",
            "Losses {'ner': 14.180403713337}\n",
            "Losses {'ner': 14.190302712460834}\n",
            "Losses {'ner': 19.060586391733523}\n",
            "Losses {'ner': 21.149501256662916}\n",
            "Losses {'ner': 25.38664782851538}\n",
            "Losses {'ner': 28.68091035733505}\n",
            "Losses {'ner': 29.808600152205145}\n",
            "Losses {'ner': 36.955728516385534}\n",
            "Losses {'ner': 43.45060753065172}\n",
            "Losses {'ner': 49.83433048886491}\n",
            "Losses {'ner': 53.72677514975203}\n",
            "Losses {'ner': 3.6410665520234033}\n",
            "Losses {'ner': 6.645169296084077}\n",
            "Losses {'ner': 9.440947173748611}\n",
            "Losses {'ner': 12.616206419644698}\n",
            "Losses {'ner': 15.113586014301802}\n",
            "Losses {'ner': 18.85776277601468}\n",
            "Losses {'ner': 23.117552421686526}\n",
            "Losses {'ner': 25.163974100135817}\n",
            "Losses {'ner': 29.356116338126583}\n",
            "Losses {'ner': 35.03657651246721}\n",
            "Losses {'ner': 39.447648209251554}\n",
            "Losses {'ner': 41.447347096454166}\n",
            "Losses {'ner': 45.652929512728576}\n",
            "Losses {'ner': 47.48300114329794}\n",
            "Losses {'ner': 4.651022587902844}\n",
            "Losses {'ner': 8.677270876354669}\n",
            "Losses {'ner': 13.677881468783227}\n",
            "Losses {'ner': 19.104992998528388}\n",
            "Losses {'ner': 24.801278992087674}\n",
            "Losses {'ner': 31.199644951729198}\n",
            "Losses {'ner': 34.52150669379992}\n",
            "Losses {'ner': 41.06604677549697}\n",
            "Losses {'ner': 42.944734729815565}\n",
            "Losses {'ner': 46.378598628984065}\n",
            "Losses {'ner': 52.15742350993244}\n",
            "Losses {'ner': 54.66297006627249}\n",
            "Losses {'ner': 55.64726713319945}\n",
            "Losses {'ner': 58.67784445170949}\n",
            "Losses {'ner': 7.321771167218685}\n",
            "Losses {'ner': 10.816386040090322}\n",
            "Losses {'ner': 12.226348724472075}\n",
            "Losses {'ner': 14.31596965275213}\n",
            "Losses {'ner': 17.51792233783027}\n",
            "Losses {'ner': 22.171816762268747}\n",
            "Losses {'ner': 27.79795212318396}\n",
            "Losses {'ner': 29.726571623033408}\n",
            "Losses {'ner': 33.092717516033474}\n",
            "Losses {'ner': 34.20324348596597}\n",
            "Losses {'ner': 38.86437790170868}\n",
            "Losses {'ner': 44.18432222022358}\n",
            "Losses {'ner': 47.1005770093264}\n",
            "Losses {'ner': 48.26835423152107}\n",
            "Losses {'ner': 7.491196879709605}\n",
            "Losses {'ner': 9.904700998093176}\n",
            "Losses {'ner': 15.256281149566348}\n",
            "Losses {'ner': 18.63918957631904}\n",
            "Losses {'ner': 20.595671782176126}\n",
            "Losses {'ner': 29.77344285314821}\n",
            "Losses {'ner': 33.06218020165274}\n",
            "Losses {'ner': 33.06294679072538}\n",
            "Losses {'ner': 36.023030276941185}\n",
            "Losses {'ner': 38.94717298867293}\n",
            "Losses {'ner': 38.94731133407838}\n",
            "Losses {'ner': 40.028278681725325}\n",
            "Losses {'ner': 47.93275222489244}\n",
            "Losses {'ner': 53.87838551732345}\n",
            "Losses {'ner': 4.783726695663063}\n",
            "Losses {'ner': 9.202033314575601}\n",
            "Losses {'ner': 10.950213222195089}\n",
            "Losses {'ner': 10.950219647656674}\n",
            "Losses {'ner': 14.654419101239661}\n",
            "Losses {'ner': 18.091010309144146}\n",
            "Losses {'ner': 18.091712913766415}\n",
            "Losses {'ner': 23.613763151573053}\n",
            "Losses {'ner': 28.854574439173135}\n",
            "Losses {'ner': 33.95254079723874}\n",
            "Losses {'ner': 37.3795541942074}\n",
            "Losses {'ner': 41.23645474353742}\n",
            "Losses {'ner': 45.02227781803222}\n",
            "Losses {'ner': 51.58817695171447}\n",
            "Losses {'ner': 5.357173710712232}\n",
            "Losses {'ner': 9.8857195007742}\n",
            "Losses {'ner': 13.991044961097202}\n",
            "Losses {'ner': 18.9770077352361}\n",
            "Losses {'ner': 25.415391285822977}\n",
            "Losses {'ner': 28.710815412907493}\n",
            "Losses {'ner': 30.72369862682325}\n",
            "Losses {'ner': 33.68578954638696}\n",
            "Losses {'ner': 35.663493645510414}\n",
            "Losses {'ner': 36.65034573522854}\n",
            "Losses {'ner': 41.382610779159506}\n",
            "Losses {'ner': 45.48169344774789}\n",
            "Losses {'ner': 48.69906909972815}\n",
            "Losses {'ner': 51.805808346094636}\n",
            "Losses {'ner': 4.34567242115736}\n",
            "Losses {'ner': 10.728375396359297}\n",
            "Losses {'ner': 14.515828564063163}\n",
            "Losses {'ner': 16.128662666959983}\n",
            "Losses {'ner': 20.64596302779607}\n",
            "Losses {'ner': 23.701113398840945}\n",
            "Losses {'ner': 29.78101310478739}\n",
            "Losses {'ner': 34.660018087320324}\n",
            "Losses {'ner': 35.71410789010331}\n",
            "Losses {'ner': 35.716195694061}\n",
            "Losses {'ner': 35.71661112657582}\n",
            "Losses {'ner': 38.210508769478956}\n",
            "Losses {'ner': 38.21053410256561}\n",
            "Losses {'ner': 40.62598535157833}\n",
            "Losses {'ner': 3.449947452842025}\n",
            "Losses {'ner': 10.324713586567668}\n",
            "Losses {'ner': 14.216252548413173}\n",
            "Losses {'ner': 17.559803175413073}\n",
            "Losses {'ner': 17.560089335615892}\n",
            "Losses {'ner': 21.35699537748497}\n",
            "Losses {'ner': 27.47920849479761}\n",
            "Losses {'ner': 35.401695634499454}\n",
            "Losses {'ner': 40.72328483548973}\n",
            "Losses {'ner': 40.723609183727596}\n",
            "Losses {'ner': 42.7638621960651}\n",
            "Losses {'ner': 48.0430214058725}\n",
            "Losses {'ner': 52.18896551060251}\n",
            "Losses {'ner': 52.18918199905186}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7hoXxcph9wp",
        "outputId": "e228e6f1-d3f0-4b1d-b23d-136d7db30c80"
      },
      "source": [
        "# Testing the NER\n",
        "\n",
        "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
        "doc = nlp(test_text)\n",
        "print(\"Entities in '%s'\" % test_text)\n",
        "for ent in doc.ents:\n",
        "  print(ent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entities in 'I ate Sushi yesterday. Maggi is a common fast food '\n",
            "I\n",
            "Sushi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "xkYFqmrYiPQ6",
        "outputId": "4be858c9-d0e0-4618-ea7c-6b31a10ad0f6"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc,style='ent',jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    I\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
              "</mark>\n",
              " ate \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sushi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
              "</mark>\n",
              " yesterday. Maggi is a common fast food </div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soAAwA4ciYRZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
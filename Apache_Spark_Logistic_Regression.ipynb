{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Machine_Learning_Training/blob/master/Apache_Spark_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgVHs6Ba0Vuu"
      },
      "source": [
        "# <font color=#FF6378> <b> Big Data Analysis with Apache Spark </font>\n",
        "\n",
        "\n",
        "## <b> Section: Spark MLLib </b>\n",
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUYR16sL0WAz"
      },
      "source": [
        "## Objective\n",
        "***\n",
        "* Logistic Regression with Pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbcbqJHl-tDM"
      },
      "source": [
        "### Logistic Regression in PySpark\n",
        "***\n",
        "\n",
        "PySpark Logistic Regression is a type of supervised machine learning model which comes under the classification type . This algorithm defines the relation among the data and classify the data according the relation among them . The logistic regression is the fundamental technique in classification that is relatively faster and easier to compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpzG4V5f0fvi"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVuySFpA2Ynv"
      },
      "outputs": [],
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzv9FyUa3eQV"
      },
      "outputs": [],
      "source": [
        "!tar xzf spark-3.2.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VdpqOqK4T1x"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIn-VVx_4RiH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XPITTiDlPBB"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ueLeL5wAkic"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql.types import * \n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col, asc,desc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.mllib.stat import Statistics\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml import Pipeline\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvrRnm-JAtbo"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcfSIS7WBJFY"
      },
      "source": [
        "Now we create an instance of SparkSession and name the app.\n",
        "\n",
        "Here we present a synthetic dataset generated using the simulator called PaySim. PaySim uses aggregated data from the private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour to later evaluate the performance of fraud detection methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsVmjTi2hrcE"
      },
      "outputs": [],
      "source": [
        "!wget -q https://www.dropbox.com/s/c99v1df5i4nvbpg/PS_20174392719_1491204439457_log.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Ob_8OkBJFd",
        "outputId": "fed9138c-218e-418a-ee4b-e29e1f96f73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
            "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
            "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
            "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 7817.71|  C90045638|      53860.0|      46042.29| M573487274|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 7107.77| C154988899|     183195.0|     176087.23| M408069119|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 7861.64|C1912850431|    176087.23|     168225.59| M633326333|           0.0|           0.0|      0|             0|\n",
            "|   1| PAYMENT| 4024.36|C1265012928|       2671.0|           0.0|M1176932104|           0.0|           0.0|      0|             0|\n",
            "|   1|   DEBIT| 5337.77| C712410124|      41720.0|      36382.23| C195600860|       41898.0|      40348.79|      0|             0|\n",
            "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark=SparkSession.builder.appName('synthetic-paysim').getOrCreate()\n",
        "  \n",
        "#create spark dataframe of input csv file\n",
        "df=spark.read.csv('PS_20174392719_1491204439457_log.csv',inferSchema=True,header=True)\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXZ1631vB3Ye",
        "outputId": "19e163ce-bfa3-4820-b1fc-7e362bcb5619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- step: integer (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- amount: double (nullable = true)\n",
            " |-- nameOrig: string (nullable = true)\n",
            " |-- oldbalanceOrg: double (nullable = true)\n",
            " |-- newbalanceOrig: double (nullable = true)\n",
            " |-- nameDest: string (nullable = true)\n",
            " |-- oldbalanceDest: double (nullable = true)\n",
            " |-- newbalanceDest: double (nullable = true)\n",
            " |-- isFraud: integer (nullable = true)\n",
            " |-- isFlaggedFraud: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z62lw0XDg89",
        "outputId": "ba9a4619-4855-4ff3-cf7f-3269a232abf8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['step',\n",
              " 'type',\n",
              " 'amount',\n",
              " 'nameOrig',\n",
              " 'oldbalanceOrg',\n",
              " 'newbalanceOrig',\n",
              " 'nameDest',\n",
              " 'oldbalanceDest',\n",
              " 'newbalanceDest',\n",
              " 'isFraud',\n",
              " 'isFlaggedFraud']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmh9rpInYVnS"
      },
      "source": [
        "We select only a few columns. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6fG1_ZQDh1D"
      },
      "outputs": [],
      "source": [
        "df = df.select(\"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"isFraud\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VwTAo-MDcXT",
        "outputId": "824e30d4-2cf2-4257-bcac-b23fc7ec9626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------+-------------+--------------+-------+\n",
            "|   type| amount|oldbalanceOrg|newbalanceOrig|isFraud|\n",
            "+-------+-------+-------------+--------------+-------+\n",
            "|PAYMENT|9839.64|     170136.0|     160296.36|      0|\n",
            "|PAYMENT|1864.28|      21249.0|      19384.72|      0|\n",
            "+-------+-------+-------------+--------------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76L3dQt6Yd8L"
      },
      "source": [
        "Now we split the data into train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vAYHDX4QwkX"
      },
      "outputs": [],
      "source": [
        "train, test = df.randomSplit([0.7, 0.3], seed=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWfyMHJCQwkY",
        "outputId": "21c860be-9398-4e7a-a2b2-4287b503d46f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set length: 4451490 records\n",
            "Test set length: 1911130 records\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train set length: {train.count()} records\")\n",
        "print(f\"Test set length: {test.count()} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqrcQQkNQwkZ",
        "outputId": "c34f5d7d-e3f1-42a1-8fac-c394d76d8b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------+-------------+--------------+-------+\n",
            "|   type|amount|oldbalanceOrg|newbalanceOrig|isFraud|\n",
            "+-------+------+-------------+--------------+-------+\n",
            "|CASH_IN|  1.42|   1270713.41|    1270714.83|      0|\n",
            "|CASH_IN|  4.35|   4136277.22|    4136281.57|      0|\n",
            "+-------+------+-------------+--------------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgA6spVxQwka"
      },
      "source": [
        "In this dataset, any column of type string is treated as a categorical feature, but sometimes we might have numeric features we want treated as categorical or vice versa. Weâ€™ll need to carefully identify which columns are numeric and which are categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du0KXJjaQwkc",
        "outputId": "79efbda3-3cbb-4784-f24d-dcc78e25937e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('type', 'string'),\n",
              " ('amount', 'double'),\n",
              " ('oldbalanceOrg', 'double'),\n",
              " ('newbalanceOrig', 'double'),\n",
              " ('isFraud', 'int')]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAzmmyEnQwkd"
      },
      "outputs": [],
      "source": [
        "catCols = [x for (x, dataType) in train.dtypes if dataType == \"string\"]\n",
        "numCols = [\n",
        "    x for (x, dataType) in train.dtypes if ((dataType == \"double\") & (x != \"isFraud\"))\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6vNyM8MQwkd",
        "outputId": "89ef60ec-b2c8-49d8-a2ad-6f623a0652e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['amount', 'oldbalanceOrg', 'newbalanceOrig']\n",
            "['type']\n"
          ]
        }
      ],
      "source": [
        "print(numCols)\n",
        "print(catCols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaqKy9XaQwkf"
      },
      "source": [
        "Now we perform one hot encoding to convert categorical column to numerical column.\n",
        "\n",
        "StringIndexer:\n",
        "Converts a single feature to an index feature.\n",
        "http://spark.apache.org/docs/latest/ml-features#stringindexer\n",
        "\n",
        "\n",
        "OneHotEncoder:\n",
        "http://spark.apache.org/docs/latest/ml-features#onehotencoder\n",
        "\n",
        "For more info: http://spark.apache.org/docs/latest/ml-features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNKVD_z8bCoT",
        "outputId": "c8e0aab8-c18b-43b7-aa4e-0a7bb8612d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|count(type)|\n",
            "+-----------+\n",
            "|          5|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train.agg(F.countDistinct(\"type\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ManBtxepbE3y",
        "outputId": "d4d946a7-101c-4639-cafc-5609bbc339e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-------+\n",
            "|    type|  count|\n",
            "+--------+-------+\n",
            "|TRANSFER| 373084|\n",
            "| CASH_IN| 979536|\n",
            "|CASH_OUT|1566112|\n",
            "| PAYMENT|1503731|\n",
            "|   DEBIT|  29027|\n",
            "+--------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train.groupBy(\"type\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsN9VknobGZd"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.feature import StringIndexer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qXn3ujnbNaN"
      },
      "outputs": [],
      "source": [
        "string_indexer = [\n",
        "    StringIndexer(inputCol=x, outputCol=x + \"_StringIndexer\", handleInvalid=\"skip\")\n",
        "    for x in catCols\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXJyH3mebPa9",
        "outputId": "7d938db1-ce48-4076-9cbf-ddad88e5f19e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[StringIndexer_6c98e72780af]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string_indexer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeet9YAzbSdh"
      },
      "outputs": [],
      "source": [
        "one_hot_encoder = [\n",
        "    OneHotEncoder(\n",
        "        inputCols=[f\"{x}_StringIndexer\" for x in catCols],\n",
        "        outputCols=[f\"{x}_OneHotEncoder\" for x in catCols],\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YojF1pNrbW72",
        "outputId": "315f4160-f207-4b9f-fe41-914a325c8590"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[OneHotEncoder_22f6ae91a9cd]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "one_hot_encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQUfvXiGbZ0h"
      },
      "source": [
        "Now we do Vector Assembly. VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Haw4uJr9bXPS"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI6REVTiQwkj"
      },
      "outputs": [],
      "source": [
        "assemblerInput = [x for x in numCols]\n",
        "assemblerInput += [f\"{x}_OneHotEncoder\" for x in catCols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBI2eUDIQwkk",
        "outputId": "2c790870-fb3c-4f77-d101-99415e16fd28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['amount', 'oldbalanceOrg', 'newbalanceOrig', 'type_OneHotEncoder']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assemblerInput"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu_4hmwUeicw"
      },
      "source": [
        "The input column will be the assembler input that has been one hot encoded and the output column will be the features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsRcq_2lQwkk"
      },
      "outputs": [],
      "source": [
        "vector_assembler = VectorAssembler(inputCols=assemblerInput, outputCol=\"VectorAssembler_features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8D9AtX-Qwkl"
      },
      "outputs": [],
      "source": [
        "stages = []\n",
        "stages += string_indexer\n",
        "stages += one_hot_encoder\n",
        "stages += [vector_assembler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4F82GKYQwkl",
        "outputId": "3150dfe6-d011-4138-f8f7-f0fa471c9e8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[StringIndexer_6c98e72780af,\n",
              " OneHotEncoder_22f6ae91a9cd,\n",
              " VectorAssembler_f735bab59364]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM4h60jSQwkl",
        "outputId": "d9237f99-ab04-409d-9f3d-fc77feb85302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 229 ms, sys: 26.3 ms, total: 255 ms\n",
            "Wall time: 27.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline().setStages(stages)\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "pp_df = model.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdLHPHwXdFBT"
      },
      "source": [
        "We can see how our features have been assembled in a vector in the last column. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRRvPQe3Qwkm",
        "outputId": "bdbffb1f-c4b9-4b74-99e3-fbc26e634fe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------+-------------+--------------+---------------------------------------------------+\n",
            "|type   |amount|oldbalanceOrg|newbalanceOrig|VectorAssembler_features                           |\n",
            "+-------+------+-------------+--------------+---------------------------------------------------+\n",
            "|CASH_IN|4.58  |94241.0      |94245.58      |[4.58,94241.0,94245.58,0.0,0.0,1.0,0.0]            |\n",
            "|CASH_IN|5.44  |0.0          |5.44          |(7,[0,2,5],[5.44,5.44,1.0])                        |\n",
            "|CASH_IN|6.07  |400680.0     |400686.07     |[6.07,400680.0,400686.07,0.0,0.0,1.0,0.0]          |\n",
            "|CASH_IN|6.76  |11322.0      |11328.76      |[6.76,11322.0,11328.76,0.0,0.0,1.0,0.0]            |\n",
            "|CASH_IN|8.27  |8428410.94   |8428419.21    |[8.27,8428410.94,8428419.21,0.0,0.0,1.0,0.0]       |\n",
            "|CASH_IN|8.44  |39384.0      |39392.44      |[8.44,39384.0,39392.44,0.0,0.0,1.0,0.0]            |\n",
            "|CASH_IN|9.04  |99971.0      |99980.04      |[9.04,99971.0,99980.04,0.0,0.0,1.0,0.0]            |\n",
            "|CASH_IN|11.13 |4116439.74   |4116450.87    |[11.13,4116439.74,4116450.87,0.0,0.0,1.0,0.0]      |\n",
            "|CASH_IN|12.79 |601743.0     |601755.79     |[12.79,601743.0,601755.79,0.0,0.0,1.0,0.0]         |\n",
            "|CASH_IN|14.36 |613030.46    |613044.82     |[14.36,613030.46,613044.82,0.0,0.0,1.0,0.0]        |\n",
            "|CASH_IN|14.4  |1.143460813E7|1.143462253E7 |[14.4,1.143460813E7,1.143462253E7,0.0,0.0,1.0,0.0] |\n",
            "|CASH_IN|14.54 |3347286.5    |3347301.03    |[14.54,3347286.5,3347301.03,0.0,0.0,1.0,0.0]       |\n",
            "|CASH_IN|15.52 |4368030.06   |4368045.59    |[15.52,4368030.06,4368045.59,0.0,0.0,1.0,0.0]      |\n",
            "|CASH_IN|17.33 |8964056.72   |8964074.05    |[17.33,8964056.72,8964074.05,0.0,0.0,1.0,0.0]      |\n",
            "|CASH_IN|17.53 |1111294.85   |1111312.37    |[17.53,1111294.85,1111312.37,0.0,0.0,1.0,0.0]      |\n",
            "|CASH_IN|22.31 |0.0          |22.31         |(7,[0,2,5],[22.31,22.31,1.0])                      |\n",
            "|CASH_IN|22.67 |405940.0     |405962.67     |[22.67,405940.0,405962.67,0.0,0.0,1.0,0.0]         |\n",
            "|CASH_IN|23.14 |1.400007828E7|1.400010142E7 |[23.14,1.400007828E7,1.400010142E7,0.0,0.0,1.0,0.0]|\n",
            "|CASH_IN|26.78 |51635.0      |51661.78      |[26.78,51635.0,51661.78,0.0,0.0,1.0,0.0]           |\n",
            "|CASH_IN|27.54 |920.0        |947.54        |[27.54,920.0,947.54,0.0,0.0,1.0,0.0]               |\n",
            "+-------+------+-------------+--------------+---------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pp_df.select(\n",
        "    \"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"VectorAssembler_features\",\n",
        ").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKI7TdRicgyN"
      },
      "source": [
        "Now we train our model using Logistic Regression. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC0w_eu5Qwkn"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Yzr_ROgHzT"
      },
      "source": [
        "Before going ahead, we will name our vector assembler column name as features as it is a pyspark convention. And we also need to label our target col name as 'label'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPBPIuQRQwkn"
      },
      "outputs": [],
      "source": [
        "data = pp_df.select(\n",
        "    F.col(\"VectorAssembler_features\").alias(\"features\"),\n",
        "    F.col(\"isFraud\").alias(\"label\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGzh4FgyQwko",
        "outputId": "36b3bab9-6fc2-4d74-addc-440a22ae031e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------+-----+\n",
            "|features                                    |label|\n",
            "+--------------------------------------------+-----+\n",
            "|[4.58,94241.0,94245.58,0.0,0.0,1.0,0.0]     |0    |\n",
            "|(7,[0,2,5],[5.44,5.44,1.0])                 |0    |\n",
            "|[6.07,400680.0,400686.07,0.0,0.0,1.0,0.0]   |0    |\n",
            "|[6.76,11322.0,11328.76,0.0,0.0,1.0,0.0]     |0    |\n",
            "|[8.27,8428410.94,8428419.21,0.0,0.0,1.0,0.0]|0    |\n",
            "+--------------------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v61dhrghgXiB"
      },
      "source": [
        "Now we fit the model on our data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHxlxrRuQwko",
        "outputId": "a2b154ef-41b6-4193-9ed8-94793575b8e3",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 491 ms, sys: 68.3 ms, total: 560 ms\n",
            "Wall time: 1min 17s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LogisticRegression().fit(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6txSC8EOQwkp",
        "outputId": "9582971f-8958-4956-b7af-eb173626dbe6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9932490712682276"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.summary.areaUnderROC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hmYd52g0A9"
      },
      "source": [
        "Thus, we get an ROC of 0.99 which is excellent. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJk4juHIQwkp",
        "outputId": "4c73a822-f1c9-4010-d416-0e98ebd2aa4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-------------------+\n",
            "|             recall|          precision|\n",
            "+-------------------+-------------------+\n",
            "|                0.0| 0.9218585005279831|\n",
            "|0.36089293096320796| 0.9218585005279831|\n",
            "| 0.4795369987598181| 0.6728538283062645|\n",
            "| 0.5527077304671352| 0.5345861655337865|\n",
            "| 0.5990078544853246| 0.4420378279438682|\n",
            "| 0.6366267052501033| 0.3797780517879161|\n",
            "| 0.6709384042992972|  0.335885761589404|\n",
            "| 0.6986357999173212| 0.3013014797646639|\n",
            "| 0.7172385283174866|   0.27168806764798|\n",
            "| 0.7391484084332369|0.24961608264693563|\n",
            "| 0.7614716825134353|0.23198992443324937|\n",
            "| 0.7759404712691195|0.21532637375243777|\n",
            "| 0.7924762298470442|0.20191700021065936|\n",
            "| 0.7945431996692849|0.18712880926881512|\n",
            "|  0.798677139313766|0.17487328023171614|\n",
            "| 0.8015708970649028|0.16397463002114165|\n",
            "| 0.8057048367093841| 0.1546579907951119|\n",
            "| 0.8081852004960728|0.14612452350698857|\n",
            "| 0.8119057461761058| 0.1387299569117751|\n",
            "| 0.8147995039272427|0.13198071514664525|\n",
            "+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        }
      ],
      "source": [
        "model.summary.pr.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ith1m3CQfwET"
      },
      "source": [
        "## Thank You !"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Apache_Spark_Logistic_Regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}